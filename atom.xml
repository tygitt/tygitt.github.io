<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ty-blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tygitt.github.io/"/>
  <updated>2019-07-22T14:44:34.961Z</updated>
  <id>http://tygitt.github.io/</id>
  
  <author>
    <name>tygitt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>apache flume</title>
    <link href="http://tygitt.github.io/2019/07/22/apache-flume/"/>
    <id>http://tygitt.github.io/2019/07/22/apache-flume/</id>
    <published>2019-07-22T14:43:57.000Z</published>
    <updated>2019-07-22T14:44:34.961Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Apache-Flume"><a href="#Apache-Flume" class="headerlink" title="Apache Flume"></a>Apache Flume</h2><p>flume官网介绍：</p><blockquote><p>​        Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.</p><p>​        Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日志数据。它具有基于流数据流的简单灵活的架构。它具有可靠的可靠性机制和许多故障转移和恢复机制，具有强大的容错性。它使用简单的可扩展数据模型，允许在线分析应用程序。</p></blockquote><p>百度百科介绍：</p><blockquote><p>​        Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p></blockquote><p><a href="http://flume.apache.org" target="_blank" rel="noopener">flume官网</a></p><hr><h4 id="flume的使用"><a href="#flume的使用" class="headerlink" title="flume的使用"></a>flume的使用</h4><p>​        flume的使用比较简单，主要是配置flume的三个组件：Source、Sink、Channel。</p><p><strong>Source</strong> 用来对接数据源，<strong>Sink</strong>用来指代数据的下沉目的地，<strong>Channel</strong>表示Source与Sink之间的管道。</p><h5 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h5><p>​        常用的Source有：Avor Source  用来对接上一个Flume agent 的Sink</p><p>​                                        Exec Source  用来对接一个给定的命令，所产生的结果。例如 tail -F  cat </p><p>​                                        Spooling Directory Source 对接一个给定的文件夹，读取其中的文件  等等。。。</p><p><strong>Sink</strong></p><p>​        常用的Sink有：     HDFS Sink : 将数据下沉到HDFS上</p><p>​                                        Hive Sink ：将数据下沉到Hive中</p><p>​                                        Logger Sink ：将数据下沉到日志中</p><p>​                                        Kafka Sink ：下沉到kafka 队列  等等。。。。</p><p><strong>Channel</strong>  一般使用 Memory Channel  把缓冲数据放在内存中。</p><p>​        如果，默认的功能，达不到要求，可以自定义组件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">通常情况下的启动命令</span></span><br><span class="line">bin/flume-ng agent -c conf -f conf/myconf.conf -n a1  -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><hr><h4 id="flume的failover和load-balance"><a href="#flume的failover和load-balance" class="headerlink" title="flume的failover和load_balance"></a>flume的failover和load_balance</h4><p>​        failover：flume的容错机制，用来配置flume的高可用，一台flume停止工作后，可以立马有另一台flume顶上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">example</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置负载均衡，然后单独配置组中的每一个节点。</span></span><br></pre></td></tr></table></figure><p>​        failover：load_balance，flume的负载均衡机制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">example</span></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = random</span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认使用的是 round_robin 轮询</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Apache-Flume&quot;&gt;&lt;a href=&quot;#Apache-Flume&quot; class=&quot;headerlink&quot; title=&quot;Apache Flume&quot;&gt;&lt;/a&gt;Apache Flume&lt;/h2&gt;&lt;p&gt;flume官网介绍：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p
      
    
    </summary>
    
    
      <category term="flume" scheme="http://tygitt.github.io/tags/flume/"/>
    
  </entry>
  
  <entry>
    <title>hadoop-mapreduce</title>
    <link href="http://tygitt.github.io/2019/07/11/hadoop-mapreduce/"/>
    <id>http://tygitt.github.io/2019/07/11/hadoop-mapreduce/</id>
    <published>2019-07-11T12:47:10.000Z</published>
    <updated>2019-07-22T14:57:26.258Z</updated>
    
    <content type="html"><![CDATA[<h3 id="hadoop-MapReduce"><a href="#hadoop-MapReduce" class="headerlink" title="hadoop-MapReduce"></a>hadoop-MapReduce</h3><p>​        MapReduce是hadoop中的一个核心组件，是分布式的计算框架。整体的理念是“分而治之”。</p><p>​        其中map表示对任务的划分，把一个大的任务划分为若干个小任务，这些小任务并且是并行执行的，几乎互不干扰。reduce表示对map阶段的结果进行汇总。得到我们想要的结果</p><h4 id="MapReduce的入门程序-wordCount"><a href="#MapReduce的入门程序-wordCount" class="headerlink" title="MapReduce的入门程序-wordCount"></a>MapReduce的入门程序-wordCount</h4><ol><li><p>自定义Mapper逻辑</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 重写map方法</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key,Text value ,Context context)</span></span>&#123;</span><br><span class="line">        String split = value.toString().split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">for</span>(String s:split)&#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(s),<span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>自定义Reducer逻辑</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 重写reduce方法</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key,Iterable&lt;IntWritable&gt; values ,Context context)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(IntWritable value : values)&#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> IntWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>组装main方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span></span>&#123;</span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(),<span class="string">"JobMain"</span>);</span><br><span class="line">        </span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"hers is a path"</span>));</span><br><span class="line">        </span><br><span class="line">        job.setMapperClass(CountMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">        </span><br><span class="line">        job.setReducerClass(CountReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        </span><br><span class="line">        job.setOutputFormatClass(TextOutputForMat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"here is output path"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(<span class="keyword">new</span> Configuration(),<span class="keyword">new</span> JobMain(),args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>​        一个简单的mapreduce只需要自定义三个类，一个mapper处理解析好的key1,value1转换为key2,value2。然后中间的步骤采用默认的处理方式，到了reducer将map阶段转化的key2,value2转换为key3,value3。然后组装main方法，执行整个程序。</p><h4 id="MapReduce的处理流程"><a href="#MapReduce的处理流程" class="headerlink" title="MapReduce的处理流程"></a>MapReduce的处理流程</h4><p><strong>1.定义InputFormat类解析文件</strong></p><p>​    第一步，将文件解析为key，value对，一般使用TextInputFormat类，经过该类解析之后的key1为文件中的每一行的行偏移量，value为每一行的内容。  类型为&lt;LongWritable,Text&gt; 。</p><p>​    如果需要自定义InputFormat类，可以通过继承FileInputFormat方法，重写其中的方法。重写createRecordReader() 、isSplitable() 方法。</p><p>​    第一个方法表示文本的读取，第二个方法表示该文件是否可切分。</p><p>​    createRecordReader()方法，需要一个RecordReader类，这个RecordReader需要自定义，通过继承RecordReader类 重写全部方法，来进行自定义。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义recordReader类</span></span><br><span class="line"><span class="meta">@override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit inputSplit,TaskAttemptContext context)</span></span>&#123;</span><br><span class="line">    <span class="comment">// 初始化方法</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>&#123;</span><br><span class="line">    <span class="comment">// 读取的核心方法 , 其中的对象，都是成员变量</span></span><br><span class="line">    <span class="keyword">if</span>(!processed) &#123;</span><br><span class="line">        fileSystem = FileSystem.get(configuration); <span class="comment">// 获得FileSystem对象</span></span><br><span class="line">        inputStream = fileSystem.open(fileSplit.getPath());<span class="comment">// 开启文件输入流</span></span><br><span class="line">        <span class="keyword">byte</span>[] bytes = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) fileSplit.getLength()];<span class="comment">// 定义字节数组</span></span><br><span class="line">        <span class="comment">// 把文件读取到字节数组中</span></span><br><span class="line">        IOUtils.readFully(inputStream, bytes, <span class="number">0</span>, (<span class="keyword">int</span>) fileSplit.getLength());</span><br><span class="line"></span><br><span class="line">        bytesWritable.set(bytes,<span class="number">0</span>, (<span class="keyword">int</span>) fileSplit.getLength());</span><br><span class="line"></span><br><span class="line">        processed = <span class="keyword">true</span>;</span><br><span class="line">        <span class="comment">// 置为true  修改标记</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> NullWritable <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>&#123;&#125;</span><br><span class="line"><span class="comment">// 返回当前key值</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>&#123;&#125;</span><br><span class="line"><span class="comment">// 返回当前value值</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>&#123;&#125;</span><br><span class="line"><span class="comment">// 查看任务进度</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;&#125;</span><br><span class="line"><span class="comment">// 做一些释放资源，关闭资源的操作</span></span><br></pre></td></tr></table></figure><p><strong>2.自定义Mapper</strong></p><p>​        自定义mapper逻辑主要是把第一步解析的key1，value1转换为key2，value2。继承Mapper类添加泛型为&lt;key1,value1,key2,value2&gt;,在写map逻辑时，首先要确认key2和value2的类型。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">key1</span>,<span class="title">value1</span>,<span class="title">key2</span>,<span class="title">Value2</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 重写map方法</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Key1 key,value1 value,Context context)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">// 逻辑处理....</span></span><br><span class="line">        context.write(key2 ,value2);</span><br><span class="line">        <span class="comment">// 写出key2 value2</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>3.分区-partitioner</strong></p><p>​        通过指定分区，可以指定相同类型的数据到同一个reduceTask中。默认使用HashPartitioner，默认的reduceTask数量为1。可以通过继承Partitioner类来自定义Partitioner类。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">key2</span>,<span class="title">value2</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 重写getPartitioner()方法</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartitioner</span><span class="params">(key2 key,value2 value, <span class="keyword">int</span> i)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 自定义分区逻辑...</span></span><br><span class="line">        <span class="keyword">return</span> (key.hashcode()&amp;Integer.MAX_VALUE)%i; <span class="comment">// 默认方式</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 可以在main方法中通过 job.setNumReduceTasks(int i) 设置reduceTask 分区的个数</span></span><br></pre></td></tr></table></figure><p><strong>4.排序-WritableComparable</strong></p><p>​        默认的排序方式是采用字典排序，需要自定义排序，可以封装一个Bean，然后实现WritableComparable<bean>接口,定义成员变量和成员方法后，重写三个方法，分别是序列化方法，反序列化方法，排序方法。对谁排序，需要把该字段放在key2上，</bean></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">JavaBean</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 成员变量，get/set方法  construct ....</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(JavaBean o)</span></span>&#123; <span class="comment">//比较方法</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.first.comparedTo(o.first);</span><br><span class="line">        <span class="comment">// 按照first变量的字典序进行排序</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">        <span class="comment">// 序列化方法</span></span><br><span class="line">        dataOutput.writeUTF(string); <span class="comment">//序列化</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">        <span class="comment">// 反序列化方法</span></span><br><span class="line">        <span class="keyword">this</span>.first = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>5.规约-Combiner</strong></p><p>​        规约是mapShuffle阶段的最后一个阶段，规约的作用就是把所在的mapTask处理的结果进行局部的聚合处理，以减少Map和Reducer之间的数据传输，提高处理效率。是MapReduce的优化手段之一。自定义Combiner需要继承Reducer类，他的泛型是&lt;k2,v2,k2,v2&gt;。Combiner与Reducer的区别，在于运行的位置。<strong>Combiner的作用就是进行局部的聚合减少map到reduce之间的网络传输量。</strong></p><p>​        Combiner能够应用的前提是不能影响最终的业务逻辑，并且Combiner输出的key/value,应该与reducer接收的key/value对应起来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义Combiner需要继承Reducer类，泛型为&lt;k2,v2,k2,v2&gt;</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">k2</span>,<span class="title">v2</span>,<span class="title">k2</span>,<span class="title">v2</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 重写reduce方法</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(k2 key,Iterable&lt;value2&gt; valu2,Context context)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 自定义逻辑。。。</span></span><br><span class="line">        context.write(key2,value2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>6.分组-WritableComparator</strong></p><p>​        分组属于reduceShuffle阶段，分组的主要作用是决定哪些数据作为一组，调用一次reduce逻辑，默认是每一个不同的key2作为的不同的组，可以通过自定义GroupingComparator继承WritableComparator类来自定分组类。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//实现自定义分组有三个步骤</span></span><br><span class="line"><span class="comment">//1.继承父类WritableComparator</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyGroup</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span></span>&#123;</span><br><span class="line">    <span class="comment">//2.调用父类构造方法</span></span><br><span class="line">    <span class="keyword">public</span> MyGroup&#123;</span><br><span class="line">        <span class="keyword">super</span>(Key2.class,<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//3.重写compare方法</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a,WritableComparable b)</span></span>&#123;</span><br><span class="line">        Key2 first = (Key2)a;</span><br><span class="line">        Key2 second = (Key2)b;</span><br><span class="line">        <span class="keyword">return</span> first.compareTo(second);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>7.reduce逻辑</strong></p><p>​        reduce阶段是把每个mapTask处理的结果进行聚合处理，将map阶段传来的key2/value2，转换为key3/value3进行输出。 一个mapreduce可以没有reduce，但必须有map。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">k2</span>,<span class="title">v2</span>,<span class="title">k3</span>,<span class="title">v3</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 重写reduce方法</span></span><br><span class="line">    <span class="meta">@override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(k2 key,iterable&lt;v2&gt; value,Context context)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">// reduce 逻辑...</span></span><br><span class="line">        context.write(key3,value3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>8.结果的输出-outputFormat</strong></p><p>​        有的时候，我们需要自定义输出类，来达到我们的业务需求。默认一般使用TextOutputFormat类</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;hadoop-MapReduce&quot;&gt;&lt;a href=&quot;#hadoop-MapReduce&quot; class=&quot;headerlink&quot; title=&quot;hadoop-MapReduce&quot;&gt;&lt;/a&gt;hadoop-MapReduce&lt;/h3&gt;&lt;p&gt;​        MapRe
      
    
    </summary>
    
    
      <category term="hadoop" scheme="http://tygitt.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>hadoop-hdfs</title>
    <link href="http://tygitt.github.io/2019/07/10/hadoop-hdfs/"/>
    <id>http://tygitt.github.io/2019/07/10/hadoop-hdfs/</id>
    <published>2019-07-10T12:54:49.000Z</published>
    <updated>2019-07-10T12:55:38.754Z</updated>
    
    <content type="html"><![CDATA[<h4 id="HDFS-hadoop分布式文件系统"><a href="#HDFS-hadoop分布式文件系统" class="headerlink" title="HDFS-hadoop分布式文件系统"></a>HDFS-hadoop分布式文件系统</h4><p>hdfs(hadoop distributed file system)分布式文件存储系统，hdfs的集群搭建，可以搭建单节点类型，也可以搭建高可用hdfs集群。</p><p>hdfs集群采用的是master/slave（主人/奴隶）架构。NameNode是主节点 DataNode是从节点</p><p>hdfs集群适合一次存储，多次读取的情形，不适合频繁的修改。因为会产生大量的元数据信息</p><p>hdfs采用分块存储，2.x版本 每个block块的大小为128M</p><p>在单节点的hdfs集群中，主要有这么几个角色：</p><ol><li>NameNode：hdfs集群中的主节点，主要负责存储元数据信息（元数据：描述数据的数据，例如名称、时间、大小、路径等信息），以及接收用户的上传下载请求。</li><li>SecondaryNameNode：辅助节点，主要负责NameNode进行元数据信息的管理，负责fsimages与edits 文件的合并。</li><li>DataNode：数据节点，主要负责具体文件的存储，提供存储功能。</li></ol><p>在高可用的hdsf集群之中，会存在两个NameNode。一个是active状态，另一个是standby状态，当第一台NameNode宕机之后，standby状态的NameNode会切换为active状态的NameNode，确保集群的高可用。  高可用状态下的hdfs集群，会有负责进行两个NameNode 信息同步的模块JournalNode，负责两个NameNode之间的信息同步，确保所有的DataNode看到的元数据信息是一致的。高可用hdfs集群，不会有secondaryNameNode的存在，被JournalNode取代。</p><p>NameNode的状态切换主要是通过两个守护进行zkfc来实现。</p><p>常用的hdfs操作命令，与linux的操作类似：</p><ul><li><p>查看指定目录下的所有的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls /(path)</span><br></pre></td></tr></table></figure></li><li><p>创建文件夹， -p表示创建parent文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /path/path/..</span><br></pre></td></tr></table></figure></li><li><p>文件的剪切，少用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mv dir</span><br></pre></td></tr></table></figure></li><li><p>文件的复制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -cp file</span><br></pre></td></tr></table></figure></li><li><p>文件的删除 ，-r表示递归的删除该路径下的所有，删除后会被收集到垃圾箱（如果配置了的话）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -rm -r /path</span><br></pre></td></tr></table></figure></li><li><p>文件的上传，把文件上传到指定的hdfs目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put file /path</span><br></pre></td></tr></table></figure></li><li><p>文件的下载，从hdfs下载到local</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -get hdfsfilr localdir</span><br></pre></td></tr></table></figure></li><li><p>文件的权限管理，-R表示递归的为此目录下所有的文件都修改权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -chmod -R 777 /path</span><br></pre></td></tr></table></figure><p>改变文件的所属用户和所属组</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -chowm -R  hadoop:hadoop /path</span><br></pre></td></tr></table></figure></li></ul><p>高级命令的使用：主要是可以对文件的大小 数量进行限制</p><ul><li><p>设置指定目录的文件上传目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setQuota 2 list</span><br></pre></td></tr></table></figure><p>表示list这个文件夹只可以有两个文件，但是文件夹本身也会有一个文件信息，所以用户只能上传一个文件</p></li><li><p>清除文件的数量上传限制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -clrQuota /path</span><br></pre></td></tr></table></figure><p>表示清除掉，该路径下的文件上传数量限制</p></li><li><p>对文件的上传大小进行限制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setSpaceQuota 4k /path</span><br></pre></td></tr></table></figure><p>表示对指定路径下的文件，进行文件大小的限制</p></li><li><p>清除文件上传的大小限制</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -clrSpaceQuota /path</span><br></pre></td></tr></table></figure><p>清除掉指定path的文件夹的文件大小上传限制</p></li></ul><p><strong>在hadoop集群搭建好之后，第一件事就是要对hdfs进行，压力的基准测试，测试hdfs集群的读写速度，和网络带宽是否足够。</strong></p><p><strong>通常hdfs集群的写入速度大约在30MB/s，读取速度大约在100MB/s</strong></p><h4 id="hdfs中的元数据管理"><a href="#hdfs中的元数据管理" class="headerlink" title="hdfs中的元数据管理"></a>hdfs中的元数据管理</h4><p>hdfs的配置信息主要在hdfs-site.xml中进行配置，</p><p>元数据的管理主要由NameNode进行管理，SecondaryNameNode进行辅助管理</p><p>首先NameNode中会有一个fsimages的文件</p><p><strong>fsimages保存着一份最为完整的元数据信息，它存在与NameNode的磁盘和内存之中，会随着元数据信息的增多而变大。</strong></p><p><strong>edies保存着最近一段时间的元数据信息的日志，主要由SecondaryNameNode进行管理</strong></p><p><strong>SecondaryNameNode：主要负责fsimages与edits文件的合并，并清空edits，合并的时机有两个条件，一个edits的大小，另一个是edits存在的时间 ，默认的大小为64M，默认的时间为1h，达到任一条件就会触发fsimages与edits文件和合并。</strong></p><h4 id="hdfs中文件的上传流程"><a href="#hdfs中文件的上传流程" class="headerlink" title="hdfs中文件的上传流程"></a>hdfs中文件的上传流程</h4><ol><li>client发出上传文件的请求，请求NameNode进行文件的上传</li><li>NameNode进行该client是否有权限进行文件的上传操作，并给予回应</li><li>收到可以进行文件上传的回复，client会请求NameNode，文件block块上传的目的地。</li><li>NameNode根据副本个数的配置（默认1个block块会由3个DataNode进行存储），返回3台DataNode的地址给客户端</li><li>客户端找到DataNode建立pipeline管道连接，进行文件的上传，文件的上传以packet包（默认64K）的方式上传到DataNode中</li><li>DataNode接受完第一个block会给Client一个ack确认机制，client进行下一个block的上传</li></ol><h4 id="hdfs中文件的下载流程"><a href="#hdfs中文件的下载流程" class="headerlink" title="hdfs中文件的下载流程"></a>hdfs中文件的下载流程</h4><ol><li>client发出文件的下载请求，请求NameNode进行文件的下载</li><li>NameNode判断client的权限，并给予回复，返回文件的部分或全部的block列表</li><li>client会找到最近的DataNode进行文件的下载，（如果客户端本身就是DataNode，就会从本机进行数据的获取，短路读取特性）</li><li>底层是建立Socket Stream 进行文件的读取</li><li>当读取完NameNode返回的block列表之后，文件还没下载完成，会继续请求NameNode接下来的block所在的DataNode</li><li>每读取完成一个block，都会进行checksum的验证，如果下载出现错误，client会从另一个DataNode进行下载，原来下载的数据，会被丢弃掉，从头开始下载。</li><li>最终读取到所有的block,合并成一个完整的文件</li></ol><p><strong>值得注意的是,文件的上传和下载,数据流不会经过NameNode,NameNode只会返回相应的响应,给出DataNode的地址。</strong></p><h4 id="hdfs的javaAPI操作"><a href="#hdfs的javaAPI操作" class="headerlink" title="hdfs的javaAPI操作"></a>hdfs的javaAPI操作</h4><p>使用javaAPI，进行hdfs的操作。</p><p>使用url进行数据的访问：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">demo1</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">// 1.注册hfds的url地址，让java能够识别hdfs的url</span></span><br><span class="line">    URL.setURLStreamHandlerFactory(<span class="keyword">new</span> FsUrlStreamHandlerFactory());</span><br><span class="line">    </span><br><span class="line">    String url = <span class="string">"hdfs://node01:8020/test/input/test.log"</span>;</span><br><span class="line">    <span class="comment">// 2.打开文件的输入流</span></span><br><span class="line">    InputStream input = <span class="keyword">new</span> URL(url).openStream();</span><br><span class="line">    <span class="comment">// 3.打开输出流 下载到本地的路径</span></span><br><span class="line">    FileOutPutStream output = <span class="keyword">new</span> FileOutPutStream(<span class="function">New <span class="title">File</span><span class="params">(String path)</span>)</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4.使用commons工具列中的copy方法 进行输入输出流的对接</span></span><br><span class="line">    IOUtils.copy(input,output);</span><br><span class="line">    <span class="comment">// 5.关闭流 使用commons的IOUtils类进行关闭流</span></span><br><span class="line">    IOUtils.closeQuietly(input);</span><br><span class="line">    IOUtils.closeQuietly(output);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 执行完毕，就会把hdfs集群中的指定url下的文件，下载到本地的路径</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>使用文件系统的方式进行hdfs上数据的访问</strong></p><ol><li><p>获得FileSystem对象，有四种方式来进行 FIleSystem对象的获取，获得FileSystem回想，剩下的就好说。</p><p><strong>获得FileSystem对象的四种方法</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改Configuration的默认FileSystem</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">configuration.set(<span class="string">"fs.defaultFS"</span>,<span class="string">"hdfs://node01:8020"</span>);</span><br><span class="line">FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line"><span class="comment">// 使用FileSystem的静态方法，传入Configuration获得FileSystem对象</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用FileSystem的静态方法传入 URI 和Configuration 在URI中指定路径</span></span><br><span class="line">URI uri = <span class="keyword">new</span> URI(<span class="string">"hdfs://node01:8020"</span>);</span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fileSystem = FileSystem.get(uri,configuration);</span><br><span class="line"><span class="comment">//传入两个参数进行对象的获取</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用FileSystem的静态方法，newInstance()方法，传入Configuration 进行对象的获取</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">configuration.set(<span class="string">"fs.defaultFS"</span>,<span class="string">"hdfs://node01:8020"</span>);</span><br><span class="line"><span class="comment">// 修改Configuration的默认文件系统，为hdfs文件系统</span></span><br><span class="line">FileSystem fileSystem = FileSystem.newInstance(configuration);</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用FileSystem的，newInstance()方法进行对象的获取</span></span><br><span class="line">URI uri = <span class="keyword">new</span> URI(<span class="string">"hdfs://node01:8020"</span>);</span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fileSystem = FileSystem.newInstance(uri,configuration);</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;HDFS-hadoop分布式文件系统&quot;&gt;&lt;a href=&quot;#HDFS-hadoop分布式文件系统&quot; class=&quot;headerlink&quot; title=&quot;HDFS-hadoop分布式文件系统&quot;&gt;&lt;/a&gt;HDFS-hadoop分布式文件系统&lt;/h4&gt;&lt;p&gt;hdfs(h
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>zookeeper总结与hadoop的安装</title>
    <link href="http://tygitt.github.io/2019/07/01/zookeeper%E6%80%BB%E7%BB%93/"/>
    <id>http://tygitt.github.io/2019/07/01/zookeeper总结/</id>
    <published>2019-07-01T13:14:55.000Z</published>
    <updated>2019-07-10T12:05:28.751Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Zookeeper的理解"><a href="#Zookeeper的理解" class="headerlink" title="Zookeeper的理解"></a>Zookeeper的理解</h3><p>​    zookeeper是apache的一个项目，是一个小型的文件协调服务。官方推荐文件存储大小不超过1MB,本质上也是一个小型的文件存储系统。</p><p>​    zookeeper在集群中的配置，通常是奇数台，原因是zookeeper集群之间存在投票选举机制。就是zookeeper集群之间，会选举出一个老大(leader),剩下的都是小弟(follower),另外还有观察者(observer)的角色，不过这个不常用。</p><p><strong>leader：zk集群中的主节点，主要是处理客户端的事务性请求（增删改）create delete set 操作。</strong></p><p><strong>follower：zk集群中的从节点，主要用来处理客户端的非事务型请求（查询），以及转发来自客户端的事务性请求。</strong></p><p>observer：观察者角色，用来处理非事务性请求（查询），转发事务性的请求。该类型节点不参与投票选举机制。</p><p>​    zookeeper在集群中的配置主要有两个地方，一个是zookeeper/conf/zoo.cfg  。 zk的默认配置文件</p><p>修改dataDir的路径.另一个要修改的就是在这个dataDir路径之中添加一个myid文件,标记该zookeeper的id.</p><h4 id="zookeeper中的主从机制与主备机制"><a href="#zookeeper中的主从机制与主备机制" class="headerlink" title="zookeeper中的主从机制与主备机制"></a>zookeeper中的主从机制与主备机制</h4><p>​    zookeeper中的主从机制：zookeeper集群中会存在一个主节点leader，剩余的都是从节点.leader节点进行任务的分配,从节点执行分配的任务.</p><p>​    zookeeper中的主备机制：zookeeper中的主备机制主要针对于集群中的主节点，备用节点主要是为了保证zk集群的24小时高可用，当leader节点宕机之后，备用节点就会转换为leader节点。确保了zk集群的高可用。</p><p><strong>zookeeper集群的基本特性：全局数据的一致性。保证每一台机器的zk中的数据都是一样的。</strong></p><h4 id="zookeeper的shell操作，以及javaAPI的操作"><a href="#zookeeper的shell操作，以及javaAPI的操作" class="headerlink" title="zookeeper的shell操作，以及javaAPI的操作"></a>zookeeper的shell操作，以及javaAPI的操作</h4><p>​    zookeeper中的每一个节点被称为znode，znode既具有文件的特性，也具有文件夹的特性，即该节点下还可以创建子节点。</p><h5 id="常用的zookeeper-shell操作："><a href="#常用的zookeeper-shell操作：" class="headerlink" title="常用的zookeeper shell操作："></a>常用的zookeeper shell操作：</h5><p>​    ls path：查看该路径下所有的znode（节点）</p><p>​    create [-s] [-e] path  [value] ：在指定路径下创建一个节点。</p><p>​                -s 表示创建一个序列化的节点  -e 表示创建一个临时节点，在client断开连接后，该节点会消失</p><p>​    set：修改覆盖节点中的内容</p><p>​    get：获得节点中的内容</p><p>​    delete：删除该节点，如果该节点下有子节点则无法删除</p><p>​    rmr：递归删除，删除该节点和该节点下的所有节点</p><p>​    get path  watch  ： 为该节点添加watch监听，监听该节点的操作。watch只会触发一次。</p><p>​    history：列出命令的历史</p><h5 id="znode节点的属性"><a href="#znode节点的属性" class="headerlink" title="znode节点的属性"></a>znode节点的属性</h5><p>​    通过get命令可以获得该节点的内容和属性。</p><p>​    znode的属性：</p><p>​        dataVersion:数据的版本号，每次对节点进行set操作，该数字会加1，避免了数据更新的顺序问题</p><p>​        cversion:子节点的版本号，子节点有变化就会加1</p><p>​        aclVersion:ACL的版本号</p><p>​        ephemeralOwner:如果该节点是临时节点，则改值表示与该节点绑定的session id 如果不是临时节点，该值为0</p><p>在client与zkServer通信之前，需要建立连接，该链接成为session</p><h5 id="zookeeper的watch机制"><a href="#zookeeper的watch机制" class="headerlink" title="zookeeper的watch机制"></a>zookeeper的watch机制</h5><p>​    zookeeper提供了 <strong>分布式数据发布/订阅</strong> 功能，一个典型的发布/订阅模型。能让多个订阅者同时监听一个发布者，当该发布者的自身状态改变时，会通知所有订阅者。</p><p>​    zookeeper中通过 <strong>watch机制实现这种分布式的通知功能</strong></p><p>​    大致上来说一个监听器Watcher的流程为：<strong>客户端向服务端注册watcher、服务端发生相应事件触发watcher、客户端回调watcher的触发事件的情况</strong></p><p>​    <strong>watch机制是一次性触发的</strong>，即触发之后，就会销毁。</p><p>​    zookeeper使用WatchedEvent对象封装服务端的事件进行传递</p><p>​    WatchedEvent包含三个基本属性：keeperState 通知状态、EventType 事件类型、Path 节点的路径</p><p>​    watcher的事件通知从服务器端发送到客户端是异步的</p><h5 id="zookeeper的javaAPI"><a href="#zookeeper的javaAPI" class="headerlink" title="zookeeper的javaAPI"></a>zookeeper的javaAPI</h5><p>​    这里采用org.apache.curator该框架进行操作。需要导入curator-framework  curator-recipes两个依赖。</p><p>​    javaAPI开发的大致流程：</p><ol><li><p>获得curatorFramework client对象，这里采用curatorFrameworkFactory工厂对象的静态方法，生成curatorFramework 对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CuratorFramework client = CuratorFrameworkFactory.newClient(String StrConnection,RetryPolicy retryPolicy);</span><br></pre></td></tr></table></figure><p>第一个参数，代表要连接的zookeeper服务器地址，第二个参数RetryPolicy表示重试策略是一个接口</p><p>这里使用的该接口的一个实现类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RetryPolicy retryPolicy = <span class="keyword">new</span> ExponentiaBackoffRetry(<span class="number">1000</span>,<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>其中第一参数为，连接失败重连的时间间隔，第二个参数为尝试重连的次数。</p></li><li><p>开启client连接</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.start();</span><br></pre></td></tr></table></figure></li><li><p>执行相应操作</p><p><strong>增加操作：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.create().createParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(String path  [,<span class="keyword">byte</span>[] value]);</span><br></pre></td></tr></table></figure><p>其中create()表示创建一个节点</p><p>createParentsIfNeeded()表示如果该节点的路径不存在，创建该路径</p><p>withMode(CreateMode)，传入一个CreateMode枚举类型，表示创建什么类型的节点，有四种类型：永久节点、临时节点、永久序列化节点、临时序列化节点。</p><p>forPath()，表示在哪个路径下创建节点，后面有一个可选参数byte[]，表示该节点的数据</p><p><strong>删除操作：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.delete().forPath(String path);</span><br></pre></td></tr></table></figure><p>删除指定path的节点</p><p><strong>查看操作：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.get().forPath(String path);</span><br></pre></td></tr></table></figure><p>返回一个byte[] 数组。 获得该路径下节点的数据</p></li><li><p>关闭客户端</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.close();</span><br></pre></td></tr></table></figure><p>释放连接</p></li></ol><p><strong>节点的watch机制</strong></p><ol><li><p>通过curatorFrameworkFactory的静态方法获得一个client</p></li><li><p>client.start();  建立连接</p></li><li><p>添加监听器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">TreeCache treeCache = <span class="keyword">new</span> TreeCache(CuratorFramework client,String path);</span><br><span class="line"><span class="comment">//添加节点的treeCache</span></span><br><span class="line">treeCache.getListenable().addListener(<span class="keyword">new</span> TreeCacheListener()&#123;</span><br><span class="line">    <span class="comment">// 使用new 接口的方法，采用匿名内部类创建一个对象</span></span><br><span class="line">    <span class="comment">// 重写该接口中的childEvent()方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">childEvent</span><span class="params">(CuratorFramework client, TreeCacheEvent event)</span> Throws Exception</span>&#123;</span><br><span class="line">        ChildData data = event.getData(); <span class="comment">// 获得监听到的数据</span></span><br><span class="line">        <span class="comment">//.... 相应的逻辑代码</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">treeCache.start(); <span class="comment">// 开始监听</span></span><br><span class="line">Thread.sleep(<span class="number">4000000</span>); <span class="comment">//让这个线程处于休眠状态，一致保持监听状态</span></span><br></pre></td></tr></table></figure><p>值得注意的是，使用javaAPI 添加watcher，该watcher可以被重复的触发，并不是触发一次就销毁</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Zookeeper的理解&quot;&gt;&lt;a href=&quot;#Zookeeper的理解&quot; class=&quot;headerlink&quot; title=&quot;Zookeeper的理解&quot;&gt;&lt;/a&gt;Zookeeper的理解&lt;/h3&gt;&lt;p&gt;​    zookeeper是apache的一个项目，是一个
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>hexo + github搭建个人博客</title>
    <link href="http://tygitt.github.io/2019/06/30/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://tygitt.github.io/2019/06/30/hexo-github搭建个人博客/</id>
    <published>2019-06-30T12:51:08.000Z</published>
    <updated>2019-06-30T13:30:43.059Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用hexo-github搭建个人博客"><a href="#使用hexo-github搭建个人博客" class="headerlink" title="使用hexo+github搭建个人博客"></a>使用hexo+github搭建个人博客</h1><p>使用github作为后台服务器，采用hexo进行博客的搭建，非常简单。大致有以下步骤</p><h4 id="1-首先本地要有安装有git，并且安装nodejs-并配置好环境变量"><a href="#1-首先本地要有安装有git，并且安装nodejs-并配置好环境变量" class="headerlink" title="1.首先本地要有安装有git，并且安装nodejs,并配置好环境变量"></a>1.首先本地要有安装有git，并且安装nodejs,并配置好环境变量</h4><h4 id="2-搞一个github账号"><a href="#2-搞一个github账号" class="headerlink" title="2.搞一个github账号"></a>2.搞一个github账号</h4><h4 id="3-在github上创建一个repository，格式为yourGitHubName-github-io"><a href="#3-在github上创建一个repository，格式为yourGitHubName-github-io" class="headerlink" title="3.在github上创建一个repository，格式为yourGitHubName.github.io"></a>3.在github上创建一个repository，格式为yourGitHubName.github.io</h4><h4 id="4-本地生成SSH，并绑定到你的github账号"><a href="#4-本地生成SSH，并绑定到你的github账号" class="headerlink" title="4.本地生成SSH，并绑定到你的github账号"></a>4.本地生成SSH，并绑定到你的github账号</h4><h4 id="5-将hexo部署到github"><a href="#5-将hexo部署到github" class="headerlink" title="5.将hexo部署到github"></a>5.将hexo部署到github</h4><h4 id="6-格式化hexo，发布文章"><a href="#6-格式化hexo，发布文章" class="headerlink" title="6.格式化hexo，发布文章"></a>6.格式化hexo，发布文章</h4><h4 id="7-查看。设置个人域名"><a href="#7-查看。设置个人域名" class="headerlink" title="7.查看。设置个人域名"></a>7.查看。设置个人域名</h4><p><img src="assets/skr.jpeg" alt="skr"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用hexo-github搭建个人博客&quot;&gt;&lt;a href=&quot;#使用hexo-github搭建个人博客&quot; class=&quot;headerlink&quot; title=&quot;使用hexo+github搭建个人博客&quot;&gt;&lt;/a&gt;使用hexo+github搭建个人博客&lt;/h1&gt;&lt;p&gt;使用
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://tygitt.github.io/2019/06/30/hello-world/"/>
    <id>http://tygitt.github.io/2019/06/30/hello-world/</id>
    <published>2019-06-30T07:26:57.832Z</published>
    <updated>2019-06-30T07:26:57.833Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
